{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Hugging Face Transformers Generating Blog Posts.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImloWljQCCcr"
      },
      "source": [
        "# https://hackernoon.com/no-code-needs-to-adapt-to-specialists-an-argument-uo1w328p    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNde_pZ8CCct"
      },
      "source": [
        "# 0. Install and Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iY9hI_8GCCcu",
        "outputId": "da2c6b30-fce7-438e-d575-9d88986ce8dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 24.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 50.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 39.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siYyogaECCcv"
      },
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq68WraMCCcv"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTsPxEJwCCcv"
      },
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8Zkd4YECCcv"
      },
      "source": [
        "sentence = 'Once Nasreddin was invited to deliver a sermon. When he got on the pulpit, he asked, Do you know what I am going to say? The audience replied \"no\", so he announced, I have no desire to speak to people who dont even know what I will be talking about! and left. The people felt embarrassed and called him back again the next day. This time, when he asked the same question, the people replied yes. So Nasreddin said, Well, since you already know what I am going to say, I wont waste any more of your time! and left.\\\n",
        "Now the people were really perplexed. They decided to try one more time and once again invited the Mulla to speak the following week. Once again he asked the same question – Do you know what I am going to say? Now the people were prepared and so half of them answered \"yes\" while the other half replied \"no\". So Nasreddin said '"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZrAAMj3CCcw"
      },
      "source": [
        "input_ids = tokenizer.encode(sentence, return_tensors='pt')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvhMq1JHCCcw",
        "outputId": "151b2e82-fe3b-444c-a032-04c063a4fe72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_ids"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 7454, 22767,   445, 25194,   373,  9392,   284,  5203,   257, 39362,\n",
              "            13,  1649,   339,  1392,   319,   262, 17472, 15544,    11,   339,\n",
              "          1965,    11,  2141,   345,   760,   644,   314,   716,  1016,   284,\n",
              "           910,    30,   383,  5386,  8712,   366,  3919,  1600,   523,   339,\n",
              "          3414,    11,   314,   423,   645,  6227,   284,  2740,   284,   661,\n",
              "           508, 17666,   772,   760,   644,   314,   481,   307,  3375,   546,\n",
              "             0,   290,  1364,    13,   383,   661,  2936, 21100,   290,  1444,\n",
              "           683,   736,   757,   262,  1306,  1110,    13,   770,   640,    11,\n",
              "           618,   339,  1965,   262,   976,  1808,    11,   262,   661,  8712,\n",
              "          3763,    13,  1406, 22767,   445, 25194,   531,    11,  3894,    11,\n",
              "          1201,   345,  1541,   760,   644,   314,   716,  1016,   284,   910,\n",
              "            11,   314, 28329,  7030,   597,   517,   286,   534,   640,     0,\n",
              "           290,  1364,    13,  3844,   262,   661,   547,  1107, 35682,   276,\n",
              "            13,  1119,  3066,   284,  1949,   530,   517,   640,   290,  1752,\n",
              "           757,  9392,   262, 19574,    64,   284,  2740,   262,  1708,  1285,\n",
              "            13,  4874,   757,   339,  1965,   262,   976,  1808,   784,  2141,\n",
              "           345,   760,   644,   314,   716,  1016,   284,   910,    30,  2735,\n",
              "           262,   661,   547,  5597,   290,   523,  2063,   286,   606,  9373,\n",
              "           366,  8505,     1,   981,   262,   584,  2063,  8712,   366,  3919,\n",
              "          1911,  1406, 22767,   445, 25194,   531,   220]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwMubrFOCCcw",
        "outputId": "652fd3ef-d7d4-4a0c-d3cb-398eaf8bca29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "source": [
        "# generate text until the output length (which includes the context length) reaches 50\n",
        "output = model.generate(input_ids, max_length=2000, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-67d56c0bb546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# generate text until the output length (which includes the context length) reaches 50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m             )\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1855\u001b[0m             )\n\u001b[1;32m   1856\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1857\u001b[0;31m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reorder_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m             \u001b[0;31m# increase cur_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m_reorder_cache\u001b[0;34m(past, beam_idx)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tuple(\n\u001b[1;32m   1003\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpast_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer_past\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m         )\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tuple(\n\u001b[1;32m   1003\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpast_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer_past\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m         )\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \"\"\"\n\u001b[1;32m   1002\u001b[0m         return tuple(\n\u001b[0;32m-> 1003\u001b[0;31m             \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpast_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer_past\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSWEcRL9CCcx",
        "outputId": "671c9210-f17b-4308-ed8c-7403904e83af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "output"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-61353e8b2d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33LDFlkpEy3V",
        "outputId": "1abe3c7a-3732-4ded-b1b8-2b001fb0a51b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "tokenizer.decode(output[0],skip_special_tokens=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Once Nasreddin was invited to deliver a sermon. When he got on the pulpit, he asked, Do you know what I am going to say? The audience replied \"no\", so he announced, I have no desire to speak to people who dont even know what I will be talking about! and left. The people felt embarrassed and called him back again the next day. This time, when he asked the same question, the people replied yes. So Nasreddin said, Well, since you already know what I am going to say, I wont waste any more of your time! and left.Now the people were really perplexed. They decided to try one more time and once again invited the Mulla to speak the following week. Once again he asked the same question – Do you know what I am going to say? Now the people were prepared and so half of them answered \"yes\" while the other half replied \"no\". So Nasreddin said \\xa0\"Well then, what do you think I should say?\" and they all replied, \"No, we don\\'t want to hear what you have to tell us!\" So he said \"Well, then what should I do?\" They all said the exact same thing: \"We dont want you to preach to us, so we will not listen to you!\"So he left again.The next time he came to the mosque he was greeted by a group of people. He asked them what they thought of his sermon and the response was \"Yes, it was a good sermon, but we are not interested in listening to it\". He then said to them \"Do you want me to come back and preach again?\" The answer was yes, and he went on to do so for the rest of the week, until the end of Ramadan.When he returned to his home, his wife asked him what he had been doing all that time.He replied that he hadn\\'t been preaching, because he didn\\'t have anything else to talk about. She said that she was sure he would be able to find something to discuss with his audience.So she told him that if he wanted to continue preaching he should come to her house and she would give him some time to think about it.And he did so.After a few days, she came home from work and found him sitting at the kitchen table with a cup of tea and a piece of bread in front of him.She asked \"What are you doing here?\" He replied with \"I have been thinking about something and I thought I would come home and talk to my audience\". She asked what that was about, to which he responded \"It is about the Prophet (peace and blessings of Allaah be upon him) and his companions and how they were treated by the pagans. I think that it is important for us to know about this and to be aware of what is going on in the world today.\"She then asked if she could sit down with him for a while and discuss what she had come up with. And he agreed and sat down beside her.Then she started talking to him about what had happened in her life, about her family, her friends and her work. Then he started telling her about his life as a Muslim and about how his family and friends had treated him as one of their own. At one point he told her that his father had died and had left his mother and brother to take care of her, while he himself was left to look after his younger brother and sister.At this point she became very emotional and started to cry, saying \"Oh my God, how could you do this to me? You are my husband, my father and my brother. How can you treat me like this? I love you so much, you are the best husband and father that I could ever ask for. Please forgive me for being so upset with you, for I know that you love me very much and you would never do anything to hurt me or my family.\"He asked her \"How can I forgive you?\" She replied by telling him \"You can\\'t forgive someone who has done you such a great injustice. You can only forgive them when they do something good for you. If they treat you badly then you can forgive and move on with your life. But if they behave in a way that makes you feel like they are treating you like an animal then that is when you need to get angry with them and tell them that they have done something very wrong.\"When she finished her speech she went into the living room and got a glass of water and poured it over his head and then left the room.A few minutes later she returned and asked for his tea again, this time she poured some water on top of it and added some sugar to make it a little sweeter. After she left he sat there and drank the tea until he fell asleep.Later that night he woke up and saw that the water had gone cold and'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2kkk7HFCCcx",
        "outputId": "7a9519d6-c7c3-4f8c-e266-5312ee8dc5f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YouTube Title: AI learns to play video games\n",
            "\n",
            "Author: Yann LeCun, University of California, San Diego, USA\n",
            "\n",
            "\n",
            "Link: http://arxiv.org/abs/1611.05821\n",
            ",\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqhAGvUZCCcx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}